{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "## load the Groq API key\n",
    "groq_api_key=os.environ['GROQ_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf files in the local directory\n",
    "def load_and_split_text(pdf_path):\n",
    "    loader = PyPDFDirectoryLoader(pdf_path)\n",
    "\n",
    "    # docs_before_split = loader.load()\n",
    "\n",
    "\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size = 512,\n",
    "    #     chunk_overlap  = 100,\n",
    "    # )\n",
    "    # docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "\n",
    "    # return docs_after_split\n",
    "    doc_pages = loader.load()\n",
    "    return doc_pages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_substring_index(text, start_marker, end_marker):\n",
    "    start_index = text.index(start_marker) + len(start_marker)\n",
    "    end_index = text.index(end_marker, start_index)\n",
    "    return text[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_metadata(text):\n",
    "    AMBITO ='Ámbito Geográfico'\n",
    "    INFORMACION='Información Detallada'\n",
    "\n",
    "    document_tags = ['Referencia','Organismo','Sector','Subsector',\n",
    "                    AMBITO,'Tipo','Destinatarios','Plazo de solicitud']\n",
    "\n",
    "    tagIndex = 0\n",
    "    metadata = {}\n",
    "    metadataInText=\"\"\n",
    "    while tagIndex < len(document_tags)-1:\n",
    "        start = document_tags[tagIndex]\n",
    "        end = document_tags[tagIndex+1]\n",
    "        if(start=='Ámbito Geográfico'):\n",
    "            metadata[start]=extract_substring_index(text,start,end).replace(AMBITO,'').replace(INFORMACION,'').strip()\n",
    "        else:\n",
    "            metadata[start]=extract_substring_index(text,start,end).strip()\n",
    "        #metadataInText = metadataInText+\", \"+start+\" es \"+metadata[start]\n",
    "        tagIndex+=1\n",
    "            \n",
    "        \n",
    "    #return [ metadata, metadataInText ]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "def download_file(url,output_path,filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:        \n",
    "        with open(output_path+\"/\"+filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "def download_linked_files(page, output_path):\n",
    "    urls=[]\n",
    "    if \"/Annots\" in page:\n",
    "        for annot in page[\"/Annots\"]:\n",
    "            annotObj = annot.get_object()\n",
    "            if(\"/A\" in annotObj):\n",
    "                uri = annotObj.get(\"/A\").get(\"/URI\")\n",
    "                if uri is not None:\n",
    "                    print(\"[+] URL Found:\", uri)\n",
    "                    urls.append(uri)\n",
    "    \n",
    "    if(not os.path.exists(output_path)):\n",
    "        os.makedirs(output_path)\n",
    "    for url in urls:\n",
    "        download_file(url, output_path, str(uuid4())+\".pdf\")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    #model_name=\"jaimevera1107/all-MiniLM-L6-v2-similarity-es\",\n",
    "    model_name=\"jinaai/jina-embeddings-v2-base-es\",\n",
    "    model_kwargs={'device':'cpu', 'trust_remote_code': True}, \n",
    "    encode_kwargs={'normalize_embeddings': False, 'attn_implementation': \"eager\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import chromadb\n",
    "from chromadbx import UUIDGenerator\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\n",
    "# Cargar modelo de embedding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear una colección en ChromaDB\n",
    "client = chromadb.PersistentClient('./db_subvenciones')\n",
    "client.delete_collection(\"ayudas\")\n",
    "collection = client.create_collection(\"ayudas\")\n",
    "pathToMetadata = './ayudas/metadatos'\n",
    "pathToText = './ayudas/texto'\n",
    "# Función para procesar un PDF\n",
    "def process_pdf(pdf_path):\n",
    "\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        limit = 1\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text().replace(\"\\n\",\" \")\n",
    "            if limit==10:\n",
    "                break\n",
    "            if (text.find(\"Ayudas e incentivos (detalle)\") > -1):\n",
    "                a = urlparse(pdf_path)\n",
    "                output_dir = pathToText+\"/\"+os.path.basename(a.path)+\"/\"+\"Page_\"+str(page_num)\n",
    "                \n",
    "                #Get metadata from page\n",
    "                \"\"\"  metadata = get_metadata(text)\n",
    "                page_metadata = metadata[0]\n",
    "                page_metadataInText = metadata[1] \"\"\"\n",
    "                \n",
    "                page_metadata = get_metadata(text)\n",
    "                download_linked_files(page, output_dir)\n",
    "    \n",
    "                splitted_text = load_and_split_text(output_dir)\n",
    "                \n",
    "                if(len(splitted_text) > 0):\n",
    "                    embeddings=[]\n",
    "                    docs=[]\n",
    "\n",
    "                    for text in splitted_text:\n",
    "                        cleanstr=text.page_content.replace(\"\\n\",\"\")\n",
    "                        #Add the metadata in text format to associate it with every chung, since I\n",
    "                        #consider it important search criteria\n",
    "                        #completestr = \"(\"+page_metadataInText+\").\"+cleanstr\n",
    "                        completestr = cleanstr\n",
    "                        docs.append(completestr)                        \n",
    "                        embeddings.append(np.array(huggingface_embeddings.embed_query(completestr)))\n",
    "                    # Agregar a ChromaDB\n",
    "                    collection.add(\n",
    "                        ids=UUIDGenerator(len(docs)),\n",
    "                        documents=docs,\n",
    "                        embeddings=embeddings,\n",
    "                        metadatas=[page_metadata]*len(docs)\n",
    "                    )\n",
    "                    \n",
    "                    limit+=1\n",
    "\n",
    "# Procesar todos los PDFs en una carpeta\n",
    "import os\n",
    "for file in os.listdir(pathToMetadata):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        process_pdf(os.path.join(pathToMetadata, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"   71572   \"\"\"  \n",
    "         # Sample question, change to other questions you are interested in.\n",
    "# Ejemplo de búsqueda\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings = np.array(huggingface_embeddings.embed_query(query)),\n",
    "    #query_texts = [query],\n",
    "    n_results=5,\n",
    "    )\n",
    "print(results)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "\n",
    "db = Chroma(client=client, collection_name=\"ayudas\",embedding_function=huggingface_embeddings)\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
