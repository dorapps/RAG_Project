{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf files in the local directory\n",
    "loader = PyPDFDirectoryLoader(\"./ayudas/\")\n",
    "\n",
    "docs_before_split = loader.load()\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size = 1100,\n",
    "#     chunk_overlap  = 100,\n",
    "# )\n",
    "# docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "\n",
    "# docs_after_split[0]\n",
    "\n",
    "# Temporal para evitar split\n",
    "docs_after_split = docs_before_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "# avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "# avg_char_after_split = avg_doc_length(docs_after_split)\n",
    "\n",
    "# print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')\n",
    "# print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={'device':'cpu'}, \n",
    "    encode_kwargs={'normalize_embeddings': True , 'max_length':512}\n",
    ")\n",
    "\n",
    "# from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "# embedding = SentenceTransformerEmbeddings(model_name=\"hiiamsid/sentence_similarity_spanish_es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = np.array(embedding.embed_query(docs_after_split[0].page_content))\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(docs_after_split, embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Ayuntamiento de Tomelloso\"\"\"  \n",
    "         # Sample question, change to other questions you are interested in.\n",
    "relevant_documents = []\n",
    "relevant_documents = vectorstore.similarity_search(query,5)\n",
    "print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query.\\n')\n",
    "for document in relevant_documents:\n",
    "    print ('*********************')\n",
    "    print(document)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote huggingface execution\n",
    "# from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# hf = HuggingFaceHub(\n",
    "#     repo_id=\"stabilityai/stablelm-2-1_6b\",\n",
    "#     model_kwargs={\"temperature\":0.1, \"max_length\":500})\n",
    "\n",
    "# query = \"\"\"What were the trends in median household income across different states in the United States between 2021 and 2022.\"\"\"  # Sample question, change to other questions you are interested in.\n",
    "# hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "#model_name = \"datificate/gpt2-small-spanish\"\n",
    "model_name=\"bigscience/bloomz-560m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=500,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate([\"Translate to English: Je t’aime.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain. chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI (model_name=\"gpt-3.5-turbo\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = \"\"\"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "\n",
    "Pregunta: {question}\n",
    "Respuesta:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=qa_template,\n",
    "                            input_variables=['context','question'])\n",
    "combine_custom_prompt='''\n",
    "Combina todas las respuestas encontradas en la respuesta final, en diferentes líneas.\n",
    "\n",
    "Text:`{context}`\n",
    "'''\n",
    "\n",
    "\n",
    "combine_prompt_template = PromptTemplate(\n",
    "    template=combine_custom_prompt, \n",
    "    input_variables=['context']\n",
    ")\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type=\"map_reduce\",\n",
    " chain_type_kwargs= {\n",
    "        \"verbose\": False,\n",
    "        \"question_prompt\": prompt,\n",
    "        \"combine_prompt\": combine_prompt_template,\n",
    "        \"combine_document_variable_name\": \"context\"})\n",
    "\n",
    "question = \"¿Cuál es el Titulo del documento?\"\n",
    "\n",
    "result=qa_chain.invoke(question)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the QA chain with our query.\n",
    "\n",
    "prompt_template =  \"\"\"Eres un asistente de preguntas y respuestas\"\n",
    "    \"utiliza la información del contexto para dar una respuesta\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "\n",
    "Pregunta: {question}\n",
    "Respuesta:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template)\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type=\"stuff\",\n",
    " chain_type_kwargs= {\"prompt\": PROMPT})\n",
    "\n",
    "question = \"\"\"¿Cuántas ayudas da el Ayuntamiento de La Pobla de Vallbona?\"\"\"\n",
    "\n",
    "result=qa_chain.invoke(question)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = result['source_documents']\n",
    "print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "print(\"*\" * 100)\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}:\\nSource file: {doc.metadata['source']}, Page: {doc.metadata['page']}\\nContent: {doc.page_content}\")\n",
    "    print(\"-\"*100)\n",
    "    print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
